import pandas as pd
import numpy
from selenium import webdriver
import re
import time
import csv
import datetime as dt
import requests

# 16번줄 pandas_csv_path 경로 바꾸기
# 24번줄 chrom 경로 바꾸기
# 84번줄 Coopangrvw_img{}.jpg 앞에 저장경로 설정하기


dt = dt.datetime.now()
pandas_csv_path = r'C:\Users\Home\PycharmProjects\pythonProject1\Review\coopang_name_id{}.csv'.format(dt.strftime("%Y_%m_%d"))
csv_test = pd.read_csv(pandas_csv_path)
coopang_all_model = csv_test['prod_id']

options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument('window-size=1920x1080')
options.add_argument("disable-gpu")
options.add_argument(f'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36')
driver = webdriver.Chrome(r'C:\Users\Home\PycharmProjects\pythonProject1\chromedriver.exe',options=options)

rvw_list = []
size_list = []
rvw_img_list = []

for j in coopang_all_model:
    page = 0
    while True:
        page = page + 1
        url = 'https://www.coupang.com/vp/product/reviews?productId=' + str(j) + '&page=' + str(page) + '&size=100&sortBy=ORDER_SCORE_ASC&ratings=&q=&viRoleCode=2&ratingSummary=true'
        driver.get(url)
        time.sleep(3)
        prod_rvw_date = driver.find_elements_by_class_name ('sdp-review__article__list__info__product-info__reg-date')
        prod_info = driver.find_elements_by_class_name     ('sdp-review__article__list__info__product-info__name')
        prod_cst_name = driver.find_elements_by_class_name ('sdp-review__article__list__info__user__name.js_reviewUserProfileImage')
        prod_size_jud = driver.find_elements_by_class_name ('sdp-review__article__list__survey')
        prod_rvw = driver.find_elements_by_class_name      ('sdp-review__article__list__review.js_reviewArticleContentContainer')
        rvw_img = driver.find_elements_by_class_name       ('sdp-review__article__list__attachment__img.js_reviewArticleListImage.js_reviewArticleCrop')
        try:
            no_data = driver.find_element_by_css_selector    ('body > div.sdp-review__article__no-review.sdp-review__article__no-review--active')
            if no_data != None:
                break
        except:
            pass
        # 텍스트 크롤링
        for prod_rvw_date_line \
                , prod_info_line \
                , prod_cst_name_line \
                , prod_size_jud_line \
                , prod_rvw_line in zip(prod_rvw_date, prod_info, prod_cst_name, prod_size_jud, prod_rvw):
            prod_rvw_date_text = prod_rvw_date_line.text
            prod_info_split_text = prod_info_line.text
            prod_cst_name_text = prod_cst_name_line.text
            prod_size_jud_split_text = prod_size_jud_line.text
            prod_rvw_text = prod_rvw_line.text
            try:
                prod_info_split = prod_info_split_text.split(',')
                prod_name = prod_info_split[0].replace(' ', '')
                prod_color = prod_info_split[1]
                prod_by_size = prod_info_split[2]
                rvw_list.append([prod_rvw_date_text
                                    , prod_cst_name_text
                                    , prod_name
                                    , prod_color
                                    , prod_by_size
                                    , prod_rvw_text])
                prod_size_jud_split = prod_size_jud_split_text.split('\n')
                cst_size = prod_size_jud_split[0]
                cst_foot_width = prod_size_jud_split[1]
                cst_size_jud = prod_size_jud_split[2]
                size_list.append([prod_rvw_date_text
                                     , prod_cst_name_text
                                     , cst_size
                                     , cst_foot_width
                                     , cst_size_jud])
            except:
                pass
        # 이미지 크롤링
        for rvw_img_one in rvw_img:
            rvw_img_URL = rvw_img_one.get_attribute('src')
            rvw_img_list.append(rvw_img_URL)
    # 이미지 저장
    n = 1
    for url in rvw_img_list:
        n = n + 1
        r = requests.get(url)
        file = open(r'C:\Users\Home\PycharmProjects\pythonProject1\Coopangimg\Coopangrvw_img{}.jpg'.format(str(j)+str(n)), 'wb')
        file.write(r.content)
        file.close()


# 텍스트 저장
Refilename ='coopangRe{}.csv'.format(dt.strftime("%Y_%m_%d"))
Sifilename ='coopangSi{}.csv'.format(dt.strftime("%Y_%m_%d"))

f = open(Refilename, 'w', encoding='utf-8', newline='')
j = open(Sifilename, 'w', encoding='utf-8', newline='')
csvWriter = csv.writer(f)
csvWriters = csv.writer(j)

csvWriter.writerow(['prod_rvw_date','prod_cst_name','prod_name','prod_color','prod_by_size','prod_rvw'])
for q in rvw_list:
    csvWriter.writerow(q)

csvWriters.writerow(['prod_rvw_date','prod_cst_name','cst_size','cst_foot_width','cst_size_jud'])
for w in size_list:
    csvWriters.writerow(w)
f.close()
